{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning Hackathon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting Up Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hopefully you have successfully set up and activated the environment. See the README.md if not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll first try and use OpenAi [Gym](https://www.gymlibrary.dev) to make a game that we can control manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing relevant modules\n",
    "import numpy as np\n",
    "import gym\n",
    "from gym.utils.play import play, PlayPlot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can create a game environment by doing `gym.make(...)`, and selecting any from a number of environment options. Some simple examples are found in the *Classic Control* section, seen here: https://www.gymlibrary.dev/environments/classic_control/. Clicking on a game, you will find information about how to import it. \n",
    "\n",
    "For this example, we will have a go at playing and training the [Cart Pole](https://www.gymlibrary.dev/environments/classic_control/cart_pole/) environment. Have a look at the documentation page for this and you'll see that the *action* space is discrete and size 2. This means that at each point, you can either apply a force left (0) or right (1). Let's first have a go at manually playing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the cart pole environment\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "\n",
    "#Â Let's play the game manually, mapping left/right movement to a/d keys.\n",
    "play(gym.make(\"CartPole-v1\", render_mode=\"rgb_array\"), keys_to_action={\n",
    "    \"a\": np.array(0), # 0 corresponds to moving the cart leftwards\n",
    "    \"d\": np.array(1), # 1 is rightwards\n",
    "}, noop=np.array(0)) # This bit maps 'no operation' (not pressing any button) to an action."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before even running this cell, you might have realised there is a problem here. The action state only contains 'left' or 'right'; in this particular example there is no action state for doing nothing, which does not transfer well to the keyboard, where a no operation state must always be selected. \n",
    "\n",
    "To get around this, I reworked the cartpole code to include a 'no force applied' state. Don't worry about the specifics of this for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.cartPole.myCartPole import CartPoleEnv\n",
    "\n",
    "# I have edited the cartpole code to attempt to include a zero state, and increased angle of termination\n",
    "env = CartPoleEnv(render_mode=\"rgb_array\")\n",
    "\n",
    "play(env, keys_to_action={\n",
    "    \"a\": np.array(0),\n",
    "    \"d\": np.array(2),\n",
    "}, noop=np.array(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hopefully this was more playable! I also increased the angle the pole falls to reset the game, and made the pole nice and long.\n",
    "\n",
    "As a bonus little game, see if you can get to the top of the mountain (it's *really* difficult ;)):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new environment using gym.make\n",
    "env = gym.make(\"MountainCar-v0\")\n",
    "\n",
    "play(gym.make(\"MountainCar-v0\", render_mode=\"rgb_array\"), keys_to_action={\n",
    "    \"a\": np.array(0),\n",
    "    \"d\": np.array(2),\n",
    "}, noop=np.array(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hopefully at this point you believe me when i say it's not too difficult to get these games up and running.\n",
    "\n",
    "Now let's walk through how to go about training a network to learn to play a game."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The robots can now play the game. But it still cannot love. It will never love."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
